name: Build Windows EXE

on:
  push:
    branches: [ main ]
  workflow_dispatch:

jobs:
  build-windows:
    runs-on: windows-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: npm

      - name: Install dependencies
        run: npm ci

      - name: Set build version
        shell: pwsh
        run: |
          $ver = "1.0.$env:GITHUB_RUN_NUMBER"
          Write-Output "BUILD_VERSION=$ver" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append

      - name: Build llama.cpp runner (llama-cli.exe)
        shell: pwsh
        run: |
          git clone --depth 1 https://github.com/ggml-org/llama.cpp.git .\_llama_cpp
          cmake -S .\_llama_cpp -B .\_llama_cpp\build -A x64 -DLLAMA_CURL=OFF
          cmake --build .\_llama_cpp\build --config Release --target llama-cli -j 2

          $candidates = @(
            ".\\_llama_cpp\\build\\bin\\Release\\llama-cli.exe",
            ".\\_llama_cpp\\build\\bin\\Release\\llama.exe",
            ".\\_llama_cpp\\build\\Release\\llama-cli.exe",
            ".\\_llama_cpp\\build\\Release\\llama.exe"
          )
          $exe = $null
          foreach ($c in $candidates) { if (Test-Path $c) { $exe = $c; break } }
          if (-not $exe) { throw "Could not find llama-cli.exe in build output" }

          New-Item -ItemType Directory -Force -Path .\native\offline-ai | Out-Null
          Copy-Item -Force $exe .\native\offline-ai\llama-cli.exe

      - name: Optional offline AI bootstrap (models)
        run: node scripts/setup-offline-ai.cjs
        env:
          OFFLINE_AI_BOOTSTRAP: '1'
          OFFLINE_AI_REASONING_MODEL: ${{ vars.OFFLINE_AI_REASONING_MODEL || '1' }}
          OFFLINE_AI_REASONING_MODEL_URL: ${{ vars.OFFLINE_AI_REASONING_MODEL_URL || 'https://huggingface.co/lmstudio-community/Qwen2.5-3B-Instruct-GGUF/resolve/a4887aa2f3430226d71668c068e4479e2a08ca4d/Qwen2.5-3B-Instruct-Q4_K_M.gguf?download=true' }}
          OFFLINE_AI_REASONING_MODEL_SHA256: ${{ vars.OFFLINE_AI_REASONING_MODEL_SHA256 || '' }}

      - name: Build EXE
        run: npm run build && npx electron-builder --win --x64 --publish never --config.extraMetadata.version=${{ env.BUILD_VERSION }}

      - name: Inspect dist output
        shell: pwsh
        run: |
          Write-Output "=== dist directory ==="
          if (Test-Path .\dist) { Get-ChildItem .\dist -Force } else { Write-Output "dist not found" }
          Write-Output "=== dist recursive ==="
          if (Test-Path .\dist) { Get-ChildItem .\dist -Recurse -Force | Select-Object FullName,Length } else { Write-Output "dist not found" }

      - name: Verify EXE exists
        shell: pwsh
        run: |
          $exes = Get-ChildItem -Path .\dist -Recurse -Filter *.exe -ErrorAction SilentlyContinue
          if (-not $exes -or $exes.Count -eq 0) {
            throw "No .exe produced under dist/"
          }

      - name: Upload EXE
        uses: actions/upload-artifact@v4
        with:
          name: Limit-Windows-${{ env.BUILD_VERSION }}-${{ github.sha }}
          path: dist/**/*.exe

